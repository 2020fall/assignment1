{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "task4-questions.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPlQrf1Xl36e"
      },
      "source": [
        "## Columbia University\n",
        "### ECBM E4040 Neural Networks and Deep Learning. Fall 2020."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPf9EDoel36f"
      },
      "source": [
        "# Assignment 1, Task 4: Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgmI3H5ml36g"
      },
      "source": [
        "### Question 1 \n",
        "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_SwS3zvl36g"
      },
      "source": [
        "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC2SYO3Zmc1R"
      },
      "source": [
        "The \"distance\" between two distributions is measured by cross entropy to describe how close these two distribution are. The smaller cross entropy is, the closer and similar the distributions are with each other.\n",
        "In softmax classifier, the math equations normalize the output by making them positive and their sum 1. They can be considered as a probability distribution. Every groundtruth label can be written as a vector like [0,0,...,0,1,0,..,.0]. So the groundtruth can be considered as a probability distribution too. Therefore, the idea that we want to minimize the loss between the prediction and the groundtruth is exactly the same as the distributions' distance theory. So cross entropy can be used in calculating the loss of softmax classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvSo5Trkl36h"
      },
      "source": [
        "### Question 2 \n",
        "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bK7jDycl36h"
      },
      "source": [
        "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXYwFKtYmgVY"
      },
      "source": [
        "From our assignment, softmax classifier is a generalization of the logistic regression classifier to multiple classes. The main difference is defintely the number of classes (2 and multiple). What's more, binary logistic regression uses sigmoid function as its loss function to maximize the log likelihood of the probability. While multi-class task uses softmax to normalize the scores to obtain an estimated distribution. Therefore, the loss function represents the distance between the estimated and the \"true\" distribution.\n",
        "\n",
        "Simply if we want to implement a n-class classifier, we can train n binary classification models. Every model determines whether the input belongs to one of the n classes. Therefore with these n models, we use the same input and got n scores among which we choose the largest one as our result.\n",
        "\n",
        "Another approach is to we train n(n-1)/2 separate binary classification models, one for each possible pair of classes. To predict the class for a new input x, we run all classifiers on and choose the class with the most votes.\n",
        "\n",
        "Deep learning models can transform data or feature vector x to x' with layers, logistic regression or softmax classifier can be a convenient final layer that maps feature vectors to a class label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKs2ffNnl36i"
      },
      "source": [
        "### Question 3\n",
        "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_THcG8vl36i"
      },
      "source": [
        "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmkgpT3Tmk2Z"
      },
      "source": [
        "In computer vision tasks, the computational resources are important because of those deep neural networks.\n",
        "\n",
        "Firstly, the ReLU activation function can accelerate the training process by reducing computational steps. According to the function, all the negative inputs are set to zeros and all the positive inputs remain the same. When we consider the backpropagation process, the ReLU function also makes it easier and faster to compute the derivatives -- the gradient is either 0 or 1 depending on the sign of the input. Therefore, a lot of computational steps are saved and the training process becomes faster.\n",
        "\n",
        "Secondly, deep neural networks means more non-linearity are applied. Compared to other activate functions such as sigmoid and tanh which may cause the vanishing gradient problem: derivatives approach zero as n (the number of repeated applications) approaches infinity, the ReLU function's derivative always return 0 or 1 which can ensure the backpropagation process.\n",
        "\n",
        "Thirdly, the ReLU function introduces sparsity effect on the network. It can not only improve the linear separablity of the data and the parameter in the networks to reduce overfitting and also make the network robust to small inputs changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9qkhiAbl36i"
      },
      "source": [
        "### Question 4\n",
        "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
        "   \n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPAnXRvl36j"
      },
      "source": [
        "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeCwzAZPnq0G"
      },
      "source": [
        "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The parameter k refers to the number of groups that a given data sample is to be split into. The procedure is often called k-fold cross-validation.\n",
        "\n",
        "The implementation details are:\n",
        "\n",
        "shuffle the dataset randomly.\n",
        "Split the dataset into k groups\n",
        "For each unique group:\n",
        "1) Take the group as a hold out or test data set\n",
        "2) Take the remaining groups as a training data set\n",
        "3) Fit a model on the training set and evaluate it on the test set\n",
        "4) Retain the evaluation score and discard the model\n",
        "Summarize the skill of the model using the sample of model evaluation scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD3qMlldl36j"
      },
      "source": [
        "### Question 5\n",
        "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnaoghxJl36k"
      },
      "source": [
        "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sb6YgDPnuQ_"
      },
      "source": [
        "My first model got a testing accuracy of near 40%. I just simply doubled the epoch number (from 10 to 20) and reduced the batch size (from 500 to 200). And I got near 48%. And I increased the epoch number again and lower down the learning rate. Finally, I got 52%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esf71NUAl36k"
      },
      "source": [
        "### Question 6\n",
        "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-mVpKv-n3AP"
      },
      "source": [
        "Perplexity, is a parameter that balances attention between local and global aspects of the data. Due to our experience, perplexity is normally set from 5 to 50 and should be smaller than the number of points. In the experiment, the results are not good when it's too large or small.\n",
        "\n",
        "The pca-preprocess in the tSNE speed up the training process and help reduce the cost.\n",
        "\n",
        "The learning rate should be set different from other model. In this process, learning rate is set to be 100 at first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73rSY_EMl36k"
      },
      "source": [
        "<p style='color:red'><strong>SOLUTION (enter a new cell below):</strong></p>"
      ]
    }
  ]
}